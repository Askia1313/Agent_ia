"""
SystÃ¨me RAG Complet pour Documents Administratifs
Avec modÃ¨le lÃ©ger local (Ollama)
"""

# Installation requise:
# pip install sentence-transformers chromadb pypdf2 ollama langchain

import os
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import PyPDF2
from pathlib import Path
from typing import List, Dict
import ollama

class RAGSystemComplet:
    def __init__(self, 
                 embedding_model="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                 llm_model="mistral:latest",
                 ollama_host=None):
        """
        Initialise le systÃ¨me RAG complet
        
        Args:
            embedding_model: ModÃ¨le pour embeddings
            llm_model: ModÃ¨le de gÃ©nÃ©ration (doit Ãªtre installÃ© dans Ollama)
            ollama_host: URL du serveur Ollama distant (ex: "http://192.168.1.100:11434")
                        Si None, utilise le serveur local
        """
        print(f"ğŸ“¥ Chargement du modÃ¨le d'embeddings: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        
        # Base de donnÃ©es vectorielle locale
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.chroma_client.get_or_create_collection(
            name="documents_administratifs",
            metadata={"description": "ProcÃ©dures administratives"}
        )
        
        # Configuration du modÃ¨le de langage distant
        self.llm_model = llm_model
        self.ollama_host = ollama_host
        
        if ollama_host:
            print(f"ğŸŒ Connexion au serveur Ollama distant: {ollama_host}")
            self.ollama_client = ollama.Client(host=ollama_host)
        else:
            print(f"ğŸ’» Utilisation du serveur Ollama local")
            self.ollama_client = ollama.Client()
        
        # VÃ©rifier la connexion et lister les modÃ¨les disponibles
        try:
            models = self.ollama_client.list()
            print(f"âœ… Ollama connectÃ©")
            print(f"\nğŸ“‹ ModÃ¨les disponibles sur le serveur:")
            if 'models' in models:
                for model in models['models']:
                    # GÃ©rer diffÃ©rents formats de rÃ©ponse
                    if isinstance(model, dict):
                        model_name = model.get('name') or model.get('model') or str(model)
                    else:
                        model_name = str(model)
                    print(f"   - {model_name}")
            else:
                print(f"   Format de rÃ©ponse: {models}")
            print(f"\nğŸ¯ ModÃ¨le sÃ©lectionnÃ©: {llm_model}")
        except Exception as e:
            print(f"âŒ Erreur lors de la liste des modÃ¨les: {e}")
            if ollama_host:
                print(f"   VÃ©rifiez que le serveur distant est accessible Ã  {ollama_host}")
            else:
                print("   Installez Ollama depuis: https://ollama.ai")
        
        print("âœ… SystÃ¨me RAG initialisÃ©")
    
    def lire_pdf(self, chemin_pdf: str) -> str:
        """Extrait le texte d'un fichier PDF"""
        texte = ""
        with open(chemin_pdf, 'rb') as fichier:
            lecteur = PyPDF2.PdfReader(fichier)
            for page in lecteur.pages:
                texte += page.extract_text() + "\n"
        return texte
    
    def lire_txt(self, chemin_txt: str) -> str:
        """Lit un fichier texte"""
        with open(chemin_txt, 'r', encoding='utf-8') as f:
            return f.read()
    
    def decouper_texte(self, texte: str, taille_chunk=500, overlap=50) -> List[str]:
        """DÃ©coupe le texte en chunks"""
        chunks = []
        debut = 0
        
        while debut < len(texte):
            fin = debut + taille_chunk
            chunk = texte[debut:fin]
            
            if fin < len(texte):
                dernier_point = chunk.rfind('.')
                if dernier_point > taille_chunk * 0.5:
                    chunk = chunk[:dernier_point + 1]
                    fin = debut + dernier_point + 1
            
            chunks.append(chunk.strip())
            debut = fin - overlap
        
        return chunks
    
    def traiter_dossier(self, chemin_dossier: str):
        """Traite tous les documents d'un dossier"""
        dossier = Path(chemin_dossier)
        documents_traites = 0
        
        print(f"\nğŸ“‚ Traitement du dossier: {chemin_dossier}")
        
        for fichier in dossier.rglob('*'):
            if not fichier.is_file():
                continue
            
            try:
                if fichier.suffix.lower() == '.pdf':
                    print(f"  ğŸ“„ Traitement PDF: {fichier.name}")
                    texte = self.lire_pdf(str(fichier))
                elif fichier.suffix.lower() in ['.txt', '.md']:
                    print(f"  ğŸ“ Traitement TXT: {fichier.name}")
                    texte = self.lire_txt(str(fichier))
                else:
                    continue
                
                chunks = self.decouper_texte(texte)
                print(f"    âœ‚ï¸  {len(chunks)} chunks crÃ©Ã©s")
                
                embeddings = self.embedding_model.encode(chunks, show_progress_bar=False)
                
                ids = [f"{fichier.stem}_{i}" for i in range(len(chunks))]
                metadatas = [
                    {
                        "source": fichier.name,
                        "chunk_id": i,
                        "type": fichier.suffix
                    } for i in range(len(chunks))
                ]
                
                self.collection.add(
                    embeddings=embeddings.tolist(),
                    documents=chunks,
                    metadatas=metadatas,
                    ids=ids
                )
                
                documents_traites += 1
                print(f"    âœ… Embeddings crÃ©Ã©s et sauvegardÃ©s")
                
            except Exception as e:
                print(f"    âŒ Erreur: {e}")
        
        print(f"\nğŸ‰ Traitement terminÃ©: {documents_traites} documents traitÃ©s")
        print(f"ğŸ“Š Total dans la base: {self.collection.count()} chunks")
    
    def rechercher_contexte(self, question: str, n_resultats=3) -> List[Dict]:
        """Recherche les passages pertinents"""
        question_embedding = self.embedding_model.encode([question])[0]
        
        resultats = self.collection.query(
            query_embeddings=[question_embedding.tolist()],
            n_results=n_resultats
        )
        
        passages = []
        for i in range(len(resultats['documents'][0])):
            passages.append({
                'texte': resultats['documents'][0][i],
                'source': resultats['metadatas'][0][i]['source'],
                'distance': resultats['distances'][0][i]
            })
        
        return passages
    
    def generer_reponse(self, question: str, n_contextes=3) -> Dict:
        """
        GÃ©nÃ¨re une rÃ©ponse complÃ¨te avec le modÃ¨le lÃ©ger
        
        Args:
            question: Question de l'utilisateur
            n_contextes: Nombre de passages Ã  utiliser comme contexte
        
        Returns:
            Dict avec la rÃ©ponse et les sources
        """
        print(f"\nğŸ” Recherche de contexte pour: {question}")
        
        # 1. Rechercher les passages pertinents
        contextes = self.rechercher_contexte(question, n_resultats=n_contextes)
        
        if not contextes:
            return {
                'reponse': "DÃ©solÃ©, je n'ai pas trouvÃ© d'information pertinente dans les documents.",
                'sources': []
            }
        
        # 2. Construire le contexte pour le LLM
        contexte_texte = "\n\n".join([
            f"[Source: {c['source']}]\n{c['texte']}" 
            for c in contextes
        ])
        
        # 3. CrÃ©er le prompt
        prompt = f"""Tu es un assistant spÃ©cialisÃ© dans les procÃ©dures administratives. RÃ©ponds Ã  la question en te basant UNIQUEMENT sur les informations fournies ci-dessous.

CONTEXTE DOCUMENTAIRE:
{contexte_texte}

QUESTION: {question}

INSTRUCTIONS:
- RÃ©ponds en franÃ§ais de maniÃ¨re claire et structurÃ©e
- Base-toi UNIQUEMENT sur les informations du contexte fourni
- Si l'information n'est pas dans le contexte, dis-le clairement
- Cite les sources entre crochets [Source: nom_document]
- Sois prÃ©cis et concis

RÃ‰PONSE:"""

        print(f"ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec {self.llm_model}...")
        
        try:
            # 4. GÃ©nÃ©rer avec Ollama (local ou distant)
            import time
            start_time = time.time()
            print(f"â³ Envoi de la requÃªte au serveur...")
            
            response = self.ollama_client.generate(
                model=self.llm_model,
                prompt=prompt,
                options={
                    'temperature': 0.3,  # RÃ©ponses plus prÃ©cises
                    'top_p': 0.9,
                    'num_predict': 500,  # Limiter la longueur de la rÃ©ponse
                }
            )
            
            elapsed = time.time() - start_time
            print(f"âœ… RÃ©ponse reÃ§ue en {elapsed:.1f}s")
            
            reponse_texte = response['response']
            
            # 5. Extraire les sources utilisÃ©es
            sources = list(set([c['source'] for c in contextes]))
            
            return {
                'reponse': reponse_texte,
                'sources': sources,
                'contextes_utilises': contextes
            }
            
        except Exception as e:
            return {
                'reponse': f"Erreur lors de la gÃ©nÃ©ration: {e}",
                'sources': []
            }
    
    def conversation_interactive(self):
        """Mode conversation interactive"""
        print("\n" + "="*60)
        print("ğŸ’¬ MODE CONVERSATION INTERACTIVE")
        print("="*60)
        print("Tapez 'exit' ou 'quitter' pour sortir\n")
        
        while True:
            question = input("â“ Votre question: ").strip()
            
            if question.lower() in ['exit', 'quitter', 'quit']:
                print("\nğŸ‘‹ Au revoir!")
                break
            
            if not question:
                continue
            
            resultat = self.generer_reponse(question)
            
            print("\n" + "="*60)
            print("ğŸ’¡ RÃ‰PONSE:")
            print("="*60)
            print(resultat['reponse'])
            
            if resultat['sources']:
                print("\nğŸ“š Sources consultÃ©es:")
                for source in resultat['sources']:
                    print(f"  - {source}")
            
            print("\n" + "-"*60 + "\n")


# ============ EXEMPLE D'UTILISATION ============

if __name__ == "__main__":
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         SYSTÃˆME RAG - PROCÃ‰DURES ADMINISTRATIVES          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # 1. Initialiser le systÃ¨me avec serveur distant
    # Remplacez par l'IP/URL de votre machine distante
    rag = RAGSystemComplet(
        llm_model="mistral:latest",  # Utiliser le nom exact du modÃ¨le disponible
        ollama_host="http://localhost:11434"  # IP de la machine distante
    )
    
    # Pour serveur local, ne pas mettre ollama_host:
    # rag = RAGSystemComplet(llm_model="llama3.2:3b")
    
    # 2. VÃ©rifier si la base contient des documents
    try:
        nb_documents = rag.collection.count()
        print(f"\nğŸ“Š Documents dans la base: {nb_documents} chunks")
        
        if nb_documents == 0:
            print("\nâš ï¸  La base de donnÃ©es semble vide!")
            print("ğŸ’¡ Si vous avez dÃ©jÃ  chargÃ© les documents avec 'agent ia.py',")
            print("   vÃ©rifiez que les deux scripts utilisent la mÃªme base de donnÃ©es.")
            print("\nğŸ” Tentative de rÃ©cupÃ©ration des donnÃ©es...")
            # Essayer de lister les collections
            collections = rag.chroma_client.list_collections()
            print(f"ğŸ“‹ Collections disponibles: {[c.name for c in collections]}")
    except Exception as e:
        print(f"âš ï¸  Erreur lors de la vÃ©rification: {e}")
        nb_documents = 0
    
    # 3. Poser une question
    print("\n" + "="*60)
    print("ğŸ§ª TEST DE QUESTION")
    print("="*60)
    
    question = "comment obternir mon casier judiciare ?"
    resultat = rag.generer_reponse(question)
    
    print(f"\nâ“ Question: {question}\n")
    print("ğŸ’¡ RÃ©ponse:")
    print(resultat['reponse'])
    
    if resultat['sources']:
        print("\nğŸ“š Sources:")
        for source in resultat['sources']:
            print(f"  - {source}")
    
    # 4. Mode conversation interactive
    # DÃ©commenter pour utiliser:
    # rag.conversation_interactive()
