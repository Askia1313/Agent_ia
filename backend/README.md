# ğŸ¤– Backend API RAG - Django

API REST Django pour un systÃ¨me RAG (Retrieval-Augmented Generation) avec ChromaDB et Ollama. Ce backend permet de traiter des documents PDF et des pages web, de crÃ©er des embeddings vectoriels, et de gÃ©nÃ©rer des rÃ©ponses contextuelles aux questions des utilisateurs.

---

## ğŸ“‹ Table des matiÃ¨res

- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture](#-architecture)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [API Endpoints](#-api-endpoints)
- [Structure du projet](#-structure-du-projet)
- [Docker](#-docker)
- [DÃ©pannage](#-dÃ©pannage)
- [Technologies utilisÃ©es](#-technologies-utilisÃ©es)

---

## âœ¨ FonctionnalitÃ©s

- **ğŸ” Recherche sÃ©mantique** : Recherche intelligente dans une base de documents vectorisÃ©s
- **ğŸ¤– GÃ©nÃ©ration de rÃ©ponses** : Utilisation d'Ollama (Mistral) pour gÃ©nÃ©rer des rÃ©ponses contextuelles
- **ğŸ“„ Traitement de documents** : Support des fichiers PDF et des pages web
- **ğŸ’¾ Base vectorielle** : ChromaDB pour le stockage et la recherche d'embeddings
- **ğŸŒ API REST** : Endpoints Django pour l'intÃ©gration frontend
- **ğŸ”’ CORS configurÃ©** : PrÃªt pour l'intÃ©gration avec des applications frontend
- **ğŸ³ Docker ready** : Configuration Docker complÃ¨te pour le dÃ©ploiement

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â”€â–¶â”‚   Django API â”‚â”€â”€â”€â”€â”€â–¶â”‚  ChromaDB   â”‚
â”‚  (Reac.js)   â”‚â—€â”€â”€â”€â”€â”€â”‚   (Backend)  â”‚â—€â”€â”€â”€â”€â”€â”‚  (Vectors)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    Ollama    â”‚
                     â”‚  (Mistral)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flux de traitement :**
1. L'utilisateur pose une question via le frontend
2. Django reÃ§oit la requÃªte et crÃ©e un embedding de la question
3. ChromaDB recherche les documents les plus pertinents
4. Ollama gÃ©nÃ¨re une rÃ©ponse basÃ©e sur le contexte trouvÃ©
5. La rÃ©ponse est retournÃ©e au frontend avec les sources

---

## ğŸ“¦ PrÃ©requis

### Logiciels requis

- **Python 3.11+** : Langage de programmation
- **Ollama** : Serveur LLM local (avec le modÃ¨le Mistral)
- **Git** : Pour cloner le projet
- **Docker** (optionnel) : Pour le dÃ©ploiement conteneurisÃ©

### Installation d'Ollama

```bash
# Windows / macOS / Linux
# TÃ©lÃ©charger depuis https://ollama.ai

# Installer le modÃ¨le Mistral
ollama pull mistral:latest

# VÃ©rifier l'installation
ollama list
```

---

## ğŸš€ Installation

### 1. Cloner le projet

```bash
git clone https://github.com/Askia1313/Agent_ia.git
cd "agent ia/backend"
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux / macOS
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. PrÃ©parer la base de donnÃ©es RAG

Avant de dÃ©marrer le serveur, vous devez  :

```bash
# Depuis le dossier parent "agent ia"
cd ..
python communication/agent_ia.py
```

Cela va :
- âœ… Charger tous les PDFs du dossier `./pdf`
- âœ… Scraper les URLs du fichier `urls.txt`
- âœ… CrÃ©er les embeddings avec Sentence Transformers
- âœ… Indexer tout dans ChromaDB (`./chroma_db`)

### 5. Appliquer les migrations Django

```bash
cd backend
python manage.py migrate
```

### 6. DÃ©marrer le serveur

```bash
python manage.py runserver
```

Le serveur dÃ©marre sur **`http://localhost:8000`**

---

## âš™ï¸ Configuration

### Variables d'environnement

CrÃ©ez un fichier `.env` Ã  la racine du backend :

```env
# Configuration Django
DEBUG=True
SECRET_KEY=votre-clÃ©-secrÃ¨te-django
ALLOWED_HOSTS=localhost,127.0.0.1

# Configuration Ollama
OLLAMA_HOST=http://localhost:11434

# Configuration ChromaDB
CHROMA_DB_PATH=../chroma_db
```

### Configuration CORS

Les origines suivantes sont autorisÃ©es par dÃ©faut (voir `backend_ia/settings.py`) :

```python
CORS_ALLOWED_ORIGINS = [
    "http://localhost:3000",      # React.js par dÃ©faut
    "http://localhost:8080",      # React.js alternatif
    "http://localhost:5173",      
    "http://127.0.0.1:3000",
    "http://127.0.0.1:8080",
    "http://127.0.0.1:5173",
]
```

Pour ajouter d'autres origines, modifiez cette liste dans `settings.py`.

### Configuration du modÃ¨le d'embeddings

Le modÃ¨le par dÃ©faut est `paraphrase-multilingual-mpnet-base-v2` (bon pour le franÃ§ais).

Pour changer le modÃ¨le, modifiez `communication/agent_ia.py` :

```python
RAGDocumentProcessor(
    model_name="sentence-transformers/autre-modele",
    llm_model="mistral:latest"
)
```

---

## ğŸ’» Utilisation

### DÃ©marrage rapide

```bash
# 1. Activer l'environnement virtuel
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/macOS

# 2. DÃ©marrer Ollama (dans un autre terminal)
ollama serve

# 3. DÃ©marrer Django
python manage.py runserver
```

### Tester l'API

```bash
# Health check
curl http://localhost:8000/api/health/

# Poser une question
curl -X POST http://localhost:8000/api/question/ \
  -H "Content-Type: application/json" \
  -d '{"question": "Comment obtenir un passeport ?", "n_resultats": 3}'
```

---


## ğŸ“ Structure du projet

```
backend/
â”œâ”€â”€ backend_ia/                 # Configuration Django
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py            # Configuration (CORS, apps, BDD)
â”‚   â”œâ”€â”€ urls.py                # Routage principal
â”‚   â”œâ”€â”€ wsgi.py                # Point d'entrÃ©e WSGI
â”‚   â””â”€â”€ asgi.py                # Point d'entrÃ©e ASGI
â”‚
â”œâ”€â”€ communication/             # Application API RAG
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ admin.py               # Interface admin Django
â”‚   â”œâ”€â”€ apps.py                # Configuration de l'app
â”‚   â”œâ”€â”€ models.py              # ModÃ¨les de donnÃ©es (vide pour l'instant)
â”‚   â”œâ”€â”€ views.py               # Endpoints API (logique mÃ©tier)
â”‚   â”œâ”€â”€ urls.py                # Routage des endpoints
â”‚   â”œâ”€â”€ agent_ia.py            # SystÃ¨me RAG (embeddings, ChromaDB, Ollama)
â”‚   â”œâ”€â”€ tests.py               # Tests unitaires
â”‚   â””â”€â”€ migrations/            # Migrations de base de donnÃ©es
â”‚
â”œâ”€â”€ manage.py                  # Utilitaire Django CLI
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ Dockerfile                 # Configuration Docker
â”œâ”€â”€ .dockerignore              # Fichiers exclus de Docker
â””â”€â”€ README.md                  # Ce fichier
```

### Fichiers clÃ©s

- **`views.py`** : Contient la logique des endpoints API
- **`agent_ia.py`** : SystÃ¨me RAG complet (embeddings, recherche, gÃ©nÃ©ration)
- **`settings.py`** : Configuration Django (CORS, apps, base de donnÃ©es)
- **`urls.py`** : DÃ©finition des routes API

---

## ğŸ³ Docker

### DÃ©marrage avec Docker Compose

Depuis le dossier parent `agent ia/` :

```bash
# Construire et dÃ©marrer tous les services
docker-compose up --build

# En arriÃ¨re-plan
docker-compose up -d

# ArrÃªter les services
docker-compose down
```

### Services Docker

Le `docker-compose.yml` dÃ©finit 3 services :

1. **backend** : API Django (port 8000)
2. **frontend** : Application Vue.js (port 80)
3. **chroma** : Base de donnÃ©es ChromaDB (port 8001)

### Configuration Docker

Le backend utilise les volumes suivants :

```yaml
volumes:
  - ./pdf:/app/pdf              # Documents PDF
  - ./chroma_db:/app/chroma_db  # Base de donnÃ©es vectorielle
```

**Variables d'environnement Docker :**

```yaml
environment:
  - OLLAMA_HOST=http://host.docker.internal:11434
  - DJANGO_SETTINGS_MODULE=backend_ia.settings
```

### Construire l'image Docker manuellement

```bash
# Depuis le dossier backend/
docker build -t backend-rag .

# Lancer le conteneur
docker run -p 8000:8000 \
  -v $(pwd)/../chroma_db:/app/chroma_db \
  -e OLLAMA_HOST=http://host.docker.internal:11434 \
  backend-rag
```



## ğŸ› ï¸ Technologies utilisÃ©es

### Backend

- **[Django 5.2.6](https://www.djangoproject.com/)** : Framework web Python
- **[django-cors-headers 4.3.1](https://github.com/adamchainz/django-cors-headers)** : Gestion CORS

### SystÃ¨me RAG

- **[Sentence Transformers 5.1.2](https://www.sbert.net/)** : ModÃ¨les d'embeddings multilingues
- **[ChromaDB 1.3.0](https://www.trychroma.com/)** : Base de donnÃ©es vectorielle
- **[Ollama 0.6.0](https://ollama.ai/)** : Serveur LLM local (Mistral)
- **[LangChain Text Splitters 1.0.0](https://python.langchain.com/)** : DÃ©coupage de texte

### Traitement de documents

- **[PyPDF2 3.0.1](https://pypdf2.readthedocs.io/)** : Extraction de texte PDF
- **[BeautifulSoup4 4.12.2](https://www.crummy.com/software/BeautifulSoup/)** : Web scraping
- **[Requests 2.31.0](https://requests.readthedocs.io/)** : RequÃªtes HTTP

### Utilitaires

- **[python-dotenv 1.0.0](https://github.com/theskumar/python-dotenv)** : Gestion des variables d'environnement

---

## ğŸ“š Documentation supplÃ©mentaire

- **[API_DOCUMENTATION.md](../API_DOCUMENTATION.md)** : Documentation dÃ©taillÃ©e des endpoints
- **[Django Documentation](https://docs.djangoproject.com/)** : Documentation officielle Django
- **[ChromaDB Documentation](https://docs.trychroma.com/)** : Guide ChromaDB
- **[Ollama Documentation](https://github.com/ollama/ollama)** : Guide Ollama

---

## ğŸ“ Notes de dÃ©veloppement

### ModÃ¨le d'embeddings

Le modÃ¨le `paraphrase-multilingual-mpnet-base-v2` est optimisÃ© pour :
- âœ… Texte multilingue (franÃ§ais, anglais, etc.)
- âœ… Recherche sÃ©mantique
- âœ… Performance sur CPU
- âœ… Taille raisonnable (~420 MB)

### Chunking des documents

Les documents sont dÃ©coupÃ©s en chunks de :
- **Taille** : 500 caractÃ¨res
- **Overlap** : 50 caractÃ¨res
- **Raison** : Ã‰quilibre entre contexte et prÃ©cision

### GÃ©nÃ©ration de rÃ©ponses

Le systÃ¨me utilise Ollama avec Mistral pour :
- GÃ©nÃ©rer des rÃ©ponses naturelles
- Citer les sources utilisÃ©es
- Rester fidÃ¨le au contexte fourni

---

## ğŸ¤ Contribution

Pour contribuer au projet :

1. Forkez le repository
2. CrÃ©ez une branche (`git checkout -b feature/amelioration`)
3. Committez vos changements (`git commit -m 'Ajout fonctionnalitÃ©'`)
4. Pushez vers la branche (`git push origin feature/amelioration`)
5. Ouvrez une Pull Request

---



- [Documentation API complÃ¨te](../API_DOCUMENTATION.md)
- [Frontend Vue.js](../Frontend/README.md)
- [Guide de dÃ©ploiement](../DEPLOYMENT.md)

